---
title: 神经网络
tags: 
grammar_cjkRuby: true
---

### 得分函数：
	
		f(x,W)=Wx+b
		W的列维度=x的行维度。b的维度等于f的维度
		
### 损失函数：
![损失函数][1]

		加1的1可以自己指定为别的δ
		计算结果比零大才有损失，小于等于零则认为没有损失
		损失函数值越大，效果越差

![一种损失函数公式][2]

		希望得到的模型是关注整体，而不是只关注某个点

![正则化][3]

		选择正则化惩罚项值较小的模型
		
### softmax分类器：
		多类别分类
		SVM输出的是一个得分值
		softmax输出的是概率
		使用softmax计算损失函数
	
Sigmoid函数：（激活函数的一种）
![sigmoid函数][4]
	
		函数的计算结果都是在0到1之间，故可以作为概率，使用其为得分函数，将原问题映射到该函数
		
![函数说明][5]

		类别概率和为1
		存在问题：当x越大时，其导数越接近于0，进行二次求导或高次求导时会出现很严重的梯度消失现象（导数为0），导致神经网络永远无法收敛或收敛及其缓慢
		
Relu函数：（新的激活函数，远优于sigmoid函数）

![relu函数][6]

		首选（默认的）激活函数
		
### 最优化：

![梯度下降][7]

![跟随梯度][8]

		bachsize通常为2的倍数（32，4，128）
		一个epoch是完整训练一次数据
		一次迭代是指训练一次bachsize
		learning rate 一次更新大小的比例约束（通常设置的比较小，比如：0.001等），通过小的学习率更新，使用大的迭代进行更好的更新。

![反向传播][9]

![举例][10]

![整体计算][11]

![门单元][12]

		加法门单元：x+y=q则q对x，y的求导都一致，是均等分配
		MAX门单元：max计算只会保留最大的参数，所以，通过计算后只会分配给最大的
		乘法门单元：xy=q则q对x和y分别求偏导其结果分别为y和x，是一种互换的感觉
		
### 神经网络模型：
![模型图][13]

		必须指定的参数：权重w的大小
		越多神经元，越能表达复杂的模型，但容易出现过拟合的风险

### 过拟合：

		训练集表现练好，测试集表现很差
		使用正则化解决：Kw^2

### 数据预处理：

		1.每个数据都减去整体数据的均值
		2.归一化到0到1之间
		3.权值初始化：使用随机函数初始化（不能是使用全零初始化或者全为某个数初始化），或使用高斯初始化，总的说都是使用随机初始化
		4.偏向初始化直接使用全0或1初始化（任意常数）
drop-out层：

		为了减少过拟合，除了输出层和最后一个隐藏层之间的神经元之间的连接，一般会使用drop-out层舍弃部分神经节点（每次迭代随机选取部分神经元“舍弃”），只会在最后使用全连接操作，这样还可以减少计算量，加快计算速度，且大大减少过拟合的风险
		
		
  [1]: ./images/1514256569980.jpg
  [2]: ./images/1514257021040.jpg
  [3]: ./images/1514257222664.jpg
  [4]: ./images/1514257472932.jpg
  [5]: ./images/1514257564587.jpg
  [6]: ./images/1514273714083.jpg
  [7]: ./images/1514258821822.jpg
  [8]: ./images/1514258998575.jpg
  [9]: ./images/1514259582440.jpg
  [10]: ./images/1514259934186.jpg
  [11]: ./images/1514261058921.jpg
  [12]: ./images/1514261128772.jpg
  [13]: ./images/1514261508529.jpg